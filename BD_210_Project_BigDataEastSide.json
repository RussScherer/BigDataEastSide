{"paragraphs":[{"text":"%sh\nwget https://raw.githubusercontent.com/dwyl/english-words/master/words3.txt \\\n-O /tmp/dictionarywords.txt\nhadoop fs -rm /tmp/dictionarywords.txt\nhadoop fs -put /tmp/dictionarywords.txt /tmp\n\nwget https://azurestorageblobs1.blob.core.windows.net/bigdataeastside/negative-words.txt -O /tmp/negative-words.txt\nhadoop fs -rm /tmp/negative-words.txt\nhadoop fs -put /tmp/negative-words.txt /tmp\n\nwget https://azurestorageblobs1.blob.core.windows.net/bigdataeastside/positive-words.txt -O /tmp/positive-words.txt\nhadoop fs -rm /tmp/positive-words.txt\nhadoop fs -put /tmp/positive-words.txt /tmp","dateUpdated":"Dec 13, 2016 12:05:43 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481583147938_881055955","id":"20161212-225227_1574609245","result":{"code":"SUCCESS","type":"TEXT","msg":"--2016-12-13 00:05:43--  https://raw.githubusercontent.com/dwyl/english-words/master/words3.txt\nResolving raw.githubusercontent.com... 151.101.52.133\nConnecting to raw.githubusercontent.com|151.101.52.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4953699 (4.7M) [text/plain]\nSaving to: “/tmp/dictionarywords.txt”\n\n     0K .......... .......... .......... .......... ..........  1% 3.34M 1s\n    50K .......... .......... .......... .......... ..........  2% 9.87M 1s\n   100K .......... .......... .......... .......... ..........  3% 13.9M 1s\n   150K .......... .......... .......... .......... ..........  4% 36.5M 1s\n   200K .......... .......... .......... .......... ..........  5% 14.5M 1s\n   250K .......... .......... .......... .......... ..........  6% 38.9M 0s\n   300K .......... .......... .......... .......... ..........  7% 14.7M 0s\n   350K .......... .......... .......... .......... ..........  8% 36.0M 0s\n   400K .......... .......... .......... .......... ..........  9% 60.8M 0s\n   450K .......... .......... .......... .......... .......... 10%  107M 0s\n   500K .......... .......... .......... .......... .......... 11% 14.5M 0s\n   550K .......... .......... .......... .......... .......... 12% 59.3M 0s\n   600K .......... .......... .......... .......... .......... 13% 45.3M 0s\n   650K .......... .......... .......... .......... .......... 14% 18.8M 0s\n   700K .......... .......... .......... .......... .......... 15% 71.4M 0s\n   750K .......... .......... .......... .......... .......... 16% 39.7M 0s\n   800K .......... .......... .......... .......... .......... 17%  122M 0s\n   850K .......... .......... .......... .......... .......... 18% 58.9M 0s\n   900K .......... .......... .......... .......... .......... 19% 19.4M 0s\n   950K .......... .......... .......... .......... .......... 20%  105M 0s\n  1000K .......... .......... .......... .......... .......... 21% 48.0M 0s\n  1050K .......... .......... .......... .......... .......... 22%  109M 0s\n  1100K .......... .......... .......... .......... .......... 23% 36.3M 0s\n  1150K .......... .......... .......... .......... .......... 24% 34.6M 0s\n  1200K .......... .......... .......... .......... .......... 25% 51.8M 0s\n  1250K .......... .......... .......... .......... .......... 26% 60.8M 0s\n  1300K .......... .......... .......... .......... .......... 27%  113M 0s\n  1350K .......... .......... .......... .......... .......... 28% 49.7M 0s\n  1400K .......... .......... .......... .......... .......... 29% 35.2M 0s\n  1450K .......... .......... .......... .......... .......... 31% 45.2M 0s\n  1500K .......... .......... .......... .......... .......... 32%  127M 0s\n  1550K .......... .......... .......... .......... .......... 33% 48.5M 0s\n  1600K .......... .......... .......... .......... .......... 34% 50.2M 0s\n  1650K .......... .......... .......... .......... .......... 35% 49.5M 0s\n  1700K .......... .......... .......... .......... .......... 36% 34.3M 0s\n  1750K .......... .......... .......... .......... .......... 37%  123M 0s\n  1800K .......... .......... .......... .......... .......... 38% 51.0M 0s\n  1850K .......... .......... .......... .......... .......... 39% 53.8M 0s\n  1900K .......... .......... .......... .......... .......... 40% 57.9M 0s\n  1950K .......... .......... .......... .......... .......... 41% 31.0M 0s\n  2000K .......... .......... .......... .......... .......... 42% 44.2M 0s\n  2050K .......... .......... .......... .......... .......... 43%  112M 0s\n  2100K .......... .......... .......... .......... .......... 44% 88.4M 0s\n  2150K .......... .......... .......... .......... .......... 45% 63.8M 0s\n  2200K .......... .......... .......... .......... .......... 46% 59.4M 0s\n  2250K .......... .......... .......... .......... .......... 47% 41.3M 0s\n  2300K .......... .......... .......... .......... .......... 48%  117M 0s\n  2350K .......... .......... .......... .......... .......... 49% 68.0M 0s\n  2400K .......... .......... .......... .......... .......... 50% 55.8M 0s\n  2450K .......... .......... .......... .......... .......... 51% 84.8M 0s\n  2500K .......... .......... .......... .......... .......... 52%  112M 0s\n  2550K .......... .......... .......... .......... .......... 53% 53.0M 0s\n  2600K .......... .......... .......... .......... .......... 54% 54.4M 0s\n  2650K .......... .......... .......... .......... .......... 55% 55.0M 0s\n  2700K .......... .......... .......... .......... .......... 56% 48.5M 0s\n  2750K .......... .......... .......... .......... .......... 57% 71.3M 0s\n  2800K .......... .......... .......... .......... .......... 58%  106M 0s\n  2850K .......... .......... .......... .......... .......... 59% 72.9M 0s\n  2900K .......... .......... .......... .......... .......... 60% 50.6M 0s\n  2950K .......... .......... .......... .......... .......... 62% 91.2M 0s\n  3000K .......... .......... .......... .......... .......... 63%  127M 0s\n  3050K .......... .......... .......... .......... .......... 64% 35.6M 0s\n  3100K .......... .......... .......... .......... .......... 65% 84.8M 0s\n  3150K .......... .......... .......... .......... .......... 66%  108M 0s\n  3200K .......... .......... .......... .......... .......... 67% 43.7M 0s\n  3250K .......... .......... .......... .......... .......... 68%  124M 0s\n  3300K .......... .......... .......... .......... .......... 69% 76.6M 0s\n  3350K .......... .......... .......... .......... .......... 70% 45.0M 0s\n  3400K .......... .......... .......... .......... .......... 71% 67.1M 0s\n  3450K .......... .......... .......... .......... .......... 72% 66.1M 0s\n  3500K .......... .......... .......... .......... .......... 73% 68.9M 0s\n  3550K .......... .......... .......... .......... .......... 74%  106M 0s\n  3600K .......... .......... .......... .......... .......... 75%  118M 0s\n  3650K .......... .......... .......... .......... .......... 76% 64.0M 0s\n  3700K .......... .......... .......... .......... .......... 77% 81.4M 0s\n  3750K .......... .......... .......... .......... .......... 78%  103M 0s\n  3800K .......... .......... .......... .......... .......... 79% 30.3M 0s\n  3850K .......... .......... .......... .......... .......... 80% 89.1M 0s\n  3900K .......... .......... .......... .......... .......... 81% 34.2M 0s\n  3950K .......... .......... .......... .......... .......... 82%  132M 0s\n  4000K .......... .......... .......... .......... .......... 83%  110M 0s\n  4050K .......... .......... .......... .......... .......... 84%  123M 0s\n  4100K .......... .......... .......... .......... .......... 85%  133M 0s\n  4150K .......... .......... .......... .......... .......... 86% 58.9M 0s\n  4200K .......... .......... .......... .......... .......... 87%  108M 0s\n  4250K .......... .......... .......... .......... .......... 88% 70.8M 0s\n  4300K .......... .......... .......... .......... .......... 89% 94.3M 0s\n  4350K .......... .......... .......... .......... .......... 90% 71.4M 0s\n  4400K .......... .......... .......... .......... .......... 91% 77.6M 0s\n  4450K .......... .......... .......... .......... .......... 93% 82.2M 0s\n  4500K .......... .......... .......... .......... .......... 94%  135M 0s\n  4550K .......... .......... .......... .......... .......... 95% 50.0M 0s\n  4600K .......... .......... .......... .......... .......... 96%  116M 0s\n  4650K .......... .......... .......... .......... .......... 97% 77.1M 0s\n  4700K .......... .......... .......... .......... .......... 98% 90.5M 0s\n  4750K .......... .......... .......... .......... .......... 99%  105M 0s\n  4800K .......... .......... .......... .......              100%  136M=0.1s\n\n2016-12-13 00:05:44 (44.0 MB/s) - “/tmp/dictionarywords.txt” saved [4953699/4953699]\n\n16/12/13 00:05:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://sandbox.hortonworks.com:8020/tmp/dictionarywords.txt' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current\n--2016-12-13 00:05:51--  https://azurestorageblobs1.blob.core.windows.net/bigdataeastside/negative-words.txt\nResolving azurestorageblobs1.blob.core.windows.net... 52.240.48.24\nConnecting to azurestorageblobs1.blob.core.windows.net|52.240.48.24|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 46299 (45K) [text/plain]\nSaving to: “/tmp/negative-words.txt”\n\n     0K .......... .......... .......... .......... .....     100%  653K=0.07s\n\n2016-12-13 00:05:52 (653 KB/s) - “/tmp/negative-words.txt” saved [46299/46299]\n\n16/12/13 00:05:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://sandbox.hortonworks.com:8020/tmp/negative-words.txt' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current\n--2016-12-13 00:05:57--  https://azurestorageblobs1.blob.core.windows.net/bigdataeastside/positive-words.txt\nResolving azurestorageblobs1.blob.core.windows.net... 52.240.48.24\nConnecting to azurestorageblobs1.blob.core.windows.net|52.240.48.24|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 20630 (20K) [text/plain]\nSaving to: “/tmp/positive-words.txt”\n\n     0K .......... ..........                                 100%  108M=0s\n\n2016-12-13 00:05:58 (108 MB/s) - “/tmp/positive-words.txt” saved [20630/20630]\n\n16/12/13 00:06:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://sandbox.hortonworks.com:8020/tmp/positive-words.txt' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current\n"},"dateCreated":"Dec 12, 2016 10:52:27 PM","dateStarted":"Dec 13, 2016 12:05:43 AM","dateFinished":"Dec 13, 2016 12:06:03 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"text":"%sh\r\n# I ran this from PuTTY ssh command line to get it to work\r\nhadoop fs -put /tmp/trumptweets/tmp/tweets_staging /tmp/trumptweets\r\n","dateUpdated":"Dec 13, 2016 1:04:59 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481583147939_880671207","id":"20161212-225227_1016598734","result":{"code":"ERROR","type":"TEXT","msg":"sh\r interpreter not found"},"dateCreated":"Dec 12, 2016 10:52:27 PM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"text":"%pyspark\n\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# Word Level Sentiment Analysis - Donald Trump Vs Hillary Clinton\n#\n# Data from twitter had the following filters: @tedcruz,@realDonaldTrump,@HillaryClinton,@BernieSanders\n\nimport re\n\ncontractions_dict = { \n\"ain't\": \"am not; are not; is not; has not; have not\",\n\"aren't\": \"are not; am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\ndef expand_contractions(x, contractions_dict=contractions_dict):\n     def replace(match):\n         return contractions_dict[match.group(0)]\n     return contractions_re.sub(replace, x)\n\n# test the function\ntoLower = 'You don\\'t need a library!'\nprint expand_contractions(toLower)\n\n# Create dataframes for dictionary and sentiment lists:\n\n# Dictionary - never ended up using this as we changed our project.\n#input_file=\"hdfs:///tmp/dictionarywords.txt\"\n#raw_dictionaryrdd=sc.textFile(input_file).map(lambda line: line.split(\",\"))\n\n#Dictionary = raw_dictionaryrdd.map(lambda p: Row(Word=p[0]))\n#Dictionary_df=sqlContext.createDataFrame(Dictionary)\n\n#sqlContext.registerDataFrameAsTable(Dictionary_df, \"dictionary\")\n\n# Negative words\ninput_file=\"hdfs:///tmp/negative-words.txt\"\nraw_negativerdd=sc.textFile(input_file).map(lambda line: line.split(\",\"))\n\nNegative = raw_negativerdd.map(lambda p: Row(Word=p[0]))\nNegative_df=sqlContext.createDataFrame(Negative)\n\nsqlContext.registerDataFrameAsTable(Negative_df, \"negative\")\n\n# Positive words\ninput_file=\"hdfs:///tmp/positive-words.txt\"\nraw_positiverdd=sc.textFile(input_file).map(lambda line: line.split(\",\"))\n\nPositive = raw_positiverdd.map(lambda p: Row(Word=p[0]))\nPositive_df=sqlContext.createDataFrame(Positive)\n\nsqlContext.registerDataFrameAsTable(Positive_df, \"positive\")\n\n#input_files=\"hdfs:///tmp/trumptweets/6*\"        \n# will only deal a working set of files. Note that although the path to these files shows \"trumptweets\" we actually gathered both Trump and Clinton Tweets\ninput_files=\"hdfs:///tmp/trumptweets/101*,hdfs:///tmp/trumptweets/103*,hdfs:///tmp/trumptweets/104*,hdfs:///tmp/trumptweets/107*,hdfs:///tmp/trumptweets/108*,hdfs:///tmp/trumptweets/109*,hdfs:///tmp/trumptweets/110*,hdfs:///tmp/trumptweets/11*,hdfs:///tmp/trumptweets/3*,hdfs:///tmp/trumptweets/5*,hdfs:///tmp/trumptweets/6*\"\n\ntext_pres_debate_tweetsrdd=sc.textFile(input_files).map(lambda line: line.split(\",\")).map(lambda p: (p[3]))\n\nsmallRdd=text_pres_debate_tweetsrdd.map(lambda line:  line.replace(\"\\\\\",\"\").replace(\"//\",\"\").replace('\"text\"','').replace(':\"','').replace('\"',''))\nsmallerRdd=smallRdd.map(lambda line: expand_contractions(line))\n#print smallerRdd.take(5)\n\n# HILLARY \n\nfilteron = \"@HillaryClinton\"\nfilteredHillaryRdd = smallerRdd.filter(lambda line: filteron in line)\n#print filteredHillaryRdd.count()\n\nHillaryRowsRdd = filteredHillaryRdd.flatMap(lambda line: line.split(\" \"))\n#print trumpRowsRdd.take(5)\n\nHillaryRowsMapRdd = HillaryRowsRdd.map(lambda line: (line,1))\n#print HillaryRowsMapRdd.take(5)\n\nHillaryRowsAggRdd = HillaryRowsMapRdd.reduceByKey(lambda x, y: x + y)\n#print HillaryRowsAggRdd.take(5)\n\nHillary = HillaryRowsAggRdd.map(lambda p: Row(Word=p[0], WordCount=int(p[1])))\nHillary_df=sqlContext.createDataFrame(Hillary)\n\nsqlContext.registerDataFrameAsTable(Hillary_df, \"hillary\")\n\n#df_hillary_positive  = sqlContext.sql(\"SELECT sum(h.WordCount) as TotalHillaryPositive from hillary h join dictionary d on h.Word = d.Word join positive p on d.Word = p.Word\").collect()\ndf_hillary_positive  = sqlContext.sql(\"SELECT sum(h.WordCount) as TotalHillaryPositive from hillary h join positive p on h.Word = p.Word\").collect()\n\nvalposfirst = df_hillary_positive[0].TotalHillaryPositive\n\nprint 'Hillary Positive Tweet Word Count:'\nprint valposfirst\nprint ''\n\n#df_hillary_negative  = sqlContext.sql(\"SELECT sum(h.WordCount) as TotalHillaryNegative from hillary h join dictionary d on h.Word = d.Word join negative n on d.Word = n.Word\").collect()\ndf_hillary_negative  = sqlContext.sql(\"SELECT sum(h.WordCount) as TotalHillaryNegative from hillary h join negative n on h.Word = n.Word\").collect()\n#neg_hillary_count = df_hillary_negative.count()\n\nvalnegfirst = df_hillary_negative[0].TotalHillaryNegative\n\nprint 'Hillary Negative Tweet Word Count:'\nprint valnegfirst\n\nratio_hillary_sentiment = float(valposfirst) / float(valnegfirst)\n\nprint ''\nprint 'Hillary Positive / Negative:'\nprint ratio_hillary_sentiment\nprint ''\n\n# DONALD\n\nfilteron = \"@realDonaldTrump\"\nfilteredTrumpRdd = smallerRdd.filter(lambda line: filteron in line)\n\ntrumpRowsRdd = filteredTrumpRdd.flatMap(lambda line: line.split(\" \"))\n\ntrumpRowsMapRdd = trumpRowsRdd.map(lambda line: (line,1))\n\ntrumpRowsAggRdd = trumpRowsMapRdd.reduceByKey(lambda x, y: x + y)\n\ntrump = trumpRowsAggRdd.map(lambda p: Row(Word=p[0], WordCount=int(p[1])))\ntrump_df=sqlContext.createDataFrame(trump)\n\nsqlContext.registerDataFrameAsTable(trump_df, \"trump\")\n\n#df_trump_positive = sqlContext.sql(\"SELECT sum(t.WordCount) as TotalTrumpPositive from trump t join dictionary d on t.Word = d.Word join positive p on d.Word = p.Word\").collect()\ndf_trump_positive = sqlContext.sql(\"SELECT sum(t.WordCount) as TotalTrumpPositive from trump t join positive p on t.Word = p.Word\").collect()\nvalposfirst = df_trump_positive[0].TotalTrumpPositive\n\nprint 'Donald Positive Tweet Word Count:'\nprint valposfirst\n\n#df_trump_negative = sqlContext.sql(\"SELECT sum(t.WordCount) as TotalTrumpNegative from trump t join dictionary d on t.Word = d.Word join negative p on d.Word = p.Word\").collect()\ndf_trump_negative = sqlContext.sql(\"SELECT sum(t.WordCount) as TotalTrumpNegative from trump t join negative p on t.Word = p.Word\").collect()\nvalnegfirst = df_trump_negative[0].TotalTrumpNegative\n\nprint ''\nprint 'Donald Negative Tweet Word Count:'\nprint valnegfirst\n\nratio_trump_sentiment = float(valposfirst) / float(valnegfirst)\n\nprint ''\nprint 'Trump Positive / Negative:'\nprint ratio_trump_sentiment","dateUpdated":"Dec 15, 2016 12:54:13 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481583147939_880671207","id":"20161212-225227_595026806","result":{"code":"SUCCESS","type":"TEXT","msg":"You do not need a library!\nHillary Positive Tweet Word Count:\n429\n\nHillary Negative Tweet Word Count:\n447\n\nHillary Positive / Negative:\n0.959731543624\n\nDonald Positive Tweet Word Count:\n6276\n\nDonald Negative Tweet Word Count:\n5906\n\nHillary Positive / Negative:\n1.06264815442\n"},"dateCreated":"Dec 12, 2016 10:52:27 PM","dateStarted":"Dec 15, 2016 12:44:53 AM","dateFinished":"Dec 15, 2016 12:50:14 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:84","focus":true},{"text":"%spark\r\nimport com.knockdata.spark.highcharts._\r\nimport com.knockdata.spark.highcharts.model._\r\n\r\nhighcharts(bank\r\n  .series(\"x\" -> \"age\", \"y\" -> avg(col(\"balance\")))\r\n  .orderBy(col(\"age\"))).plot()","dateUpdated":"Dec 13, 2016 4:58:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql","tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481583147939_880671207","id":"20161212-225227_1931668010","result":{"code":"ERROR","type":"TEXT","msg":"spark\r interpreter not found"},"dateCreated":"Dec 12, 2016 10:52:27 PM","dateStarted":"Dec 13, 2016 6:37:44 AM","dateFinished":"Dec 13, 2016 6:37:44 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481607661666_268256591","id":"20161213-054101_1185952962","dateCreated":"Dec 13, 2016 5:41:01 AM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:86"}],"name":"BD_210_Project_BigDataEastSide3","id":"2C46F24RV","angularObjects":{"2BCKC8XEM":[],"2BCX68DXU":[],"2BFBT15Z1":[],"2BESYJRTZ":[],"2BCBP9D5X":[],"2BDTPNC5U":[],"2BF9DMDST":[],"2BEA4YGRP":[],"2BEP4GTHY":[],"2BD3W8Y9X":[],"2BEJY9WFY":[],"2BCSDJUE3":[],"2BEPFKEXV":[]},"config":{"looknfeel":"default"},"info":{}}